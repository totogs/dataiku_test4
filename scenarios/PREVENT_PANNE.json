{
  "projectKey": "LOGSANALYSIS",
  "id": "PREVENT_PANNE",
  "type": "step_based",
  "name": "prevent_panne",
  "active": false,
  "versionTag": {
    "versionNumber": 55,
    "lastModifiedBy": {
      "login": "tony"
    },
    "lastModifiedOn": 1630403200368
  },
  "checklists": {
    "checklists": []
  },
  "delayedTriggersBehavior": {
    "delayWhileRunning": true,
    "squashDelayedTriggers": true,
    "suppressTriggersWhileRunning": true
  },
  "tags": [],
  "triggers": [
    {
      "id": "rTlWI9h7",
      "type": "temporal",
      "name": "Time-based",
      "delay": 5,
      "active": true,
      "params": {
        "repeatFrequency": 150,
        "frequency": "Minutely",
        "startingFrom": "2021-08-24T13:29:00.000Z",
        "daysOfWeek": [
          "Tuesday"
        ],
        "minute": 29,
        "hour": 15,
        "timezone": "SERVER"
      }
    }
  ],
  "reporters": [],
  "params": {
    "steps": [
      {
        "id": "build_0_true_d_data_genrerated_prediction",
        "type": "build_flowitem",
        "name": "prediction_generated",
        "runConditionType": "DISABLED",
        "runConditionStatuses": [
          "SUCCESS",
          "WARNING"
        ],
        "runConditionExpression": "",
        "resetScenarioStatus": false,
        "delayBetweenRetries": 10,
        "maxRetriesOnFail": 0,
        "params": {
          "builds": [
            {
              "type": "DATASET",
              "itemId": "data_genrerated_prediction",
              "partitionsSpec": ""
            }
          ],
          "jobType": "NON_RECURSIVE_FORCED_BUILD",
          "refreshHiveMetastore": true,
          "proceedOnFailure": false
        }
      },
      {
        "id": "custom_python_hAmYJ8S1/U5k9vZ/x9jqAg",
        "type": "custom_python",
        "name": "add data to json stacked",
        "runConditionType": "DISABLED",
        "runConditionStatuses": [
          "SUCCESS",
          "WARNING"
        ],
        "runConditionExpression": "",
        "resetScenarioStatus": false,
        "delayBetweenRetries": 10,
        "maxRetriesOnFail": 0,
        "params": {
          "script": "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\n\n# Read recipe inputs\ndata_generated \u003d dataiku.Dataset(\"data_generated\")\ndata_generated_df \u003d data_generated.get_dataframe()\n\n# Read recipe inputs\njson_stacked \u003d dataiku.Dataset(\"json_stacked_distinct\")\njson_stacked_df \u003d json_stacked.get_dataframe()\n\njson_stacked_df \u003d json_stacked_df.append(data_generated_df)\n\njson_stacked.write_with_schema(json_stacked_df)",
          "envSelection": {
            "envMode": "INHERIT"
          },
          "proceedOnFailure": false
        }
      },
      {
        "id": "build_0_true_d_json_stacked_distinct_f_dHoUQGRB",
        "type": "build_flowitem",
        "name": "Retrain model",
        "runConditionType": "DISABLED",
        "runConditionStatuses": [
          "SUCCESS",
          "WARNING"
        ],
        "runConditionExpression": "",
        "resetScenarioStatus": false,
        "delayBetweenRetries": 10,
        "maxRetriesOnFail": 0,
        "params": {
          "builds": [],
          "jobType": "NON_RECURSIVE_FORCED_BUILD",
          "refreshHiveMetastore": true,
          "proceedOnFailure": false
        }
      },
      {
        "id": "build_1_true_f_dHoUQGRB",
        "type": "build_flowitem",
        "name": "Step #5",
        "runConditionType": "DISABLED",
        "runConditionStatuses": [
          "SUCCESS",
          "WARNING"
        ],
        "runConditionExpression": "",
        "resetScenarioStatus": false,
        "delayBetweenRetries": 10,
        "maxRetriesOnFail": 0,
        "params": {
          "builds": [
            {
              "type": "MANAGED_FOLDER",
              "itemId": "dHoUQGRB",
              "partitionsSpec": ""
            }
          ],
          "jobType": "NON_RECURSIVE_FORCED_BUILD",
          "refreshHiveMetastore": true,
          "proceedOnFailure": false
        }
      },
      {
        "id": "custom_python_p5E8A9uuc2HUZ9JWByekVQ",
        "type": "custom_python",
        "name": "Retrain model",
        "runConditionType": "RUN_IF_STATUS_MATCH",
        "runConditionStatuses": [
          "SUCCESS",
          "WARNING"
        ],
        "runConditionExpression": "",
        "resetScenarioStatus": false,
        "delayBetweenRetries": 10,
        "maxRetriesOnFail": 0,
        "params": {
          "script": "# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\n\n# Read recipe inputs\ndata \u003d dataiku.Dataset(\"new_train_data\")\ndf \u003d data.get_dataframe()\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nimport math\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import Sequence\nfrom datetime import timedelta\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nimport numpy as np\nimport pandas as pd\nimport time\n\nimport os\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# # Split en dataset de Train et Test\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n# Split into training, test datasets.\n# Since it\u0027s timeseries we should do it by date.\ntest_cutoff_date \u003d df[\u0027date\u0027].max() - timedelta(days\u003d7)\n\ntest_df \u003d df[df[\u0027date\u0027] \u003e test_cutoff_date]\ntrain_df \u003d df[df[\u0027date\u0027] \u003c\u003d test_cutoff_date]\n\n#check out the datasets\nprint(\u0027Test dates: {} to {}\u0027.format(test_df[\u0027date\u0027].min(), test_df[\u0027date\u0027].max()))\nprint(\u0027Train dates: {} to {}\u0027.format(train_df[\u0027date\u0027].min(), train_df[\u0027date\u0027].max()))\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\ntrain_df  \u003d train_df.set_index(\u0027date\u0027)\ntest_df \u003d test_df.set_index(\u0027date\u0027)\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\ntrain \u003d train_df\nscalers\u003d{}\nfor i in train_df.columns:\n    scaler \u003d MinMaxScaler(feature_range\u003d(-1,1))\n    s_s \u003d scaler.fit_transform(train[i].values.reshape(-1,1))\n    s_s\u003dnp.reshape(s_s,len(s_s))\n    scalers[\u0027scaler_\u0027+ i] \u003d scaler\n    train[i]\u003ds_s\ntest \u003d test_df\nfor i in test_df.columns:\n    scaler \u003d scalers[\u0027scaler_\u0027+i]\n    s_s \u003d scaler.transform(test[i].values.reshape(-1,1))\n    s_s\u003dnp.reshape(s_s,len(s_s))\n    scalers[\u0027scaler_\u0027+i] \u003d scaler\n    test[i]\u003ds_s\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# # Construction des mini-batch\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\ndef split_series(series, n_past, n_future):\n  #\n  # n_past \u003d\u003d\u003e no of past observations\n  #\n  # n_future \u003d\u003d\u003e no of future observations\n  #\n  X, y \u003d list(), list()\n  for window_start in range(len(series)):\n    past_end \u003d window_start + n_past\n    future_end \u003d past_end + n_future\n    if future_end \u003e len(series):\n      break\n    # slicing the past and future parts of the window\n    past, future \u003d series[window_start:past_end, :], series[past_end:future_end, :]\n    X.append(past)\n    y.append(future)\n  return np.array(X), np.array(y)\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nn_past \u003d 10\nn_future \u003d 5\nn_features \u003d 10\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nX_train, y_train \u003d split_series(train.values,n_past, n_future)\nX_train \u003d X_train.reshape((X_train.shape[0], X_train.shape[1],n_features))\ny_train \u003d y_train.reshape((y_train.shape[0], y_train.shape[1], n_features))\nX_test, y_test \u003d split_series(test.values,n_past, n_future)\nX_test \u003d X_test.reshape((X_test.shape[0], X_test.shape[1],n_features))\ny_test \u003d y_test.reshape((y_test.shape[0], y_test.shape[1], n_features))\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# # Création du modèle de forecasting\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nencoder_inputs \u003d tf.keras.layers.Input(shape\u003d(n_past, n_features))\nencoder \u003d tf.keras.layers.LSTM(100, return_state\u003dTrue)\nencoder_outputs \u003d encoder(encoder_inputs)\n\nencoder_states \u003d encoder_outputs[1:]\n\n#\ndecoder_inputs \u003d tf.keras.layers.RepeatVector(n_future)(encoder_outputs[0])\n\n#\ndecoder \u003d tf.keras.layers.LSTM(100, return_sequences\u003dTrue)(decoder_inputs,initial_state \u003d encoder_states)\ndecoder_outputs \u003d tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features))(decoder)\n\n#\nmodel \u003d tf.keras.models.Model(encoder_inputs,decoder_outputs)\n\n#\nmodel.summary()\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# # Apprentissage du modèle\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nreduce_lr \u003d tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\nmodel.compile(optimizer\u003dtf.keras.optimizers.Adam(), loss\u003dtf.keras.losses.Huber(), metrics\u003d[tf.keras.metrics.CosineSimilarity(), tf.keras.metrics.MeanAbsoluteError()])\nhistory \u003d model.fit(X_train,y_train,epochs\u003d25,validation_data\u003d(X_test,y_test),batch_size\u003d32,verbose\u003d0,callbacks\u003d[reduce_lr])\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nprint(history.history.keys())\nhistory.history[\"val_mean_absolute_error\"]\n\ndata_metrics \u003d {}\n\nfor key, val in history.history.items():\n    data_metrics[key] \u003d [val[-1]]\n\nprint(data_metrics)\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# # Stockage du modèle dans le folder\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nmodel_json \u003d model.to_json()\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\n# Write recipe outputs\nmodel_folder \u003d dataiku.Folder(\"dHoUQGRB\")\nmodel_folder_info \u003d model_folder.get_info()\n\nnow \u003d time.time()\n\nmodel_folder.write_json(str(now)+\"/model_json\", model_json)\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nmodel_folder.list_paths_in_partition()\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: MARKDOWN\n# # Calcul du drift du nouveau modèle\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nmetrics \u003d dataiku.Dataset(\"Metrics\")\ndf_metrics \u003d metrics.get_dataframe()\n\ndata_metrics[\"time\"] \u003d [now]\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\njson_actual_model \u003d model_folder.read_json(\"actual/model_json\")\nactual_model \u003d keras.models.model_from_json(json_actual_model)\nactual_model.compile(optimizer\u003dtf.keras.optimizers.Adam(), loss\u003dtf.keras.losses.Huber(), metrics\u003d[tf.keras.metrics.CosineSimilarity(), tf.keras.metrics.MeanAbsoluteError()])\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nresults \u003d model.evaluate(X_test, y_test, batch_size\u003d32)\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nval_mean_absolute_error \u003d results[2]\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nif data_metrics[\"val_mean_absolute_error\"] \u003c val_mean_absolute_error:\n    \n    data_metrics[\"used\"] \u003d [\"True\"]\n    df_metrics[\"used\"] \u003d df_metrics[\"used\"].where(df_metrics[\"used\"]\u003d\u003d\"True\", \"False\")\n    model_folder.write_json(\"actual/model_json\", model_json)\nelse:\n    \n    data_metrics[\"used\"] \u003d [\"False\"]\n    \ndf_metrics \u003d df_metrics.append(pd.DataFrame(data_metrics))\n\n# -------------------------------------------------------------------------------- NOTEBOOK-CELL: CODE\nmetrics.write_with_schema(df_metrics)\n",
          "envSelection": {
            "envMode": "EXPLICIT_ENV",
            "envName": "asl_alphabet"
          },
          "proceedOnFailure": false
        }
      }
    ]
  },
  "automationLocal": false,
  "customFields": {}
}