{
  "name": "forecast_metrics",
  "endpoints": [
    {
      "envSelection": {
        "envMode": "EXPLICIT_ENV",
        "envName": "asl_alphabet"
      },
      "testQueries": [
        {
          "q": {}
        }
      ],
      "inputFolderRefs": [
        {
          "ref": "dHoUQGRB"
        }
      ],
      "userFunctionName": "api_py_function",
      "code": "\n# Insert here initialization code\n\n# This function is the entry point for your Python function API\n# You can call this function with passing \"param1\", \"param2\",\n# \"param3\" as arguments to the API call\n# (see REST API documentation)\n\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\n\nimport math\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import Sequence\nfrom datetime import timedelta\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nimport numpy as np\nimport pandas as pd\nimport time\nimport json\n\nimport os\n\n\n\ndef api_py_function():\n    \n    client \u003d DSSClient(\"http://34.142.63.173:11000\", \"eJIRjKPvY6bYTcsdC1e2d1JUmGbrBIF2\")\n    \n    dss_projects \u003d client.list_project_keys()\n    print(dss_projects)\n    \n    # Read recipe inputs\n    data_generated \u003d dataiku.Dataset(\"data_generated\", project_key\u003d\"LOGSANALYSIS\")\n    data_generated_df \u003d data_generated.get_dataframe()\n\n    df_cpy  \u003d df.set_index(\u0027date\u0027)\n\n    data \u003d df_cpy\n    scalers\u003d{}\n    for i in df_cpy.columns:\n        scaler \u003d MinMaxScaler(feature_range\u003d(-1,1))\n        s_s \u003d scaler.fit_transform(data[i].values.reshape(-1,1))\n        s_s\u003dnp.reshape(s_s,len(s_s))\n        scalers[\u0027scaler_\u0027+ i] \u003d scaler\n        data_to_pred[i]\u003ds_s\n\n\n    data_to_pred \u003d data.values\n    data_to_pred \u003d data_to_pred.reshape((1, data_to_pred.shape[0],data_to_pred.shape[1]))\n    \n    handle \u003d dataiku.Folder(\"model_forecast\")\n    json_config \u003d handle.read_json(\"model_json\")\n    model \u003d keras.models.model_from_json(json_config)\n    \n    \n    data_pred \u003d model.predict(data_to_pred)\n\n\n    for index,i in enumerate(df_cpy.columns):\n        scaler \u003d scalers[\u0027scaler_\u0027+i]\n        data_pred[:,:,index]\u003dscaler.inverse_transform(data_pred[:,:,index])\n\n\n    data_pred \u003d data_pred.squeeze()\n\n\n    df_pred \u003d pd.DataFrame(data_pred, columns\u003ddf_cpy.columns)\n\n\n    lastdate \u003d df[\"date\"].max()\n    listdate \u003d []\n    for i in range(len(val)):\n        lastdate \u003d lastdate + timedelta(minutes\u003d15)\n        listdate.append(lastdate)\n\n    df_pred[\"date\"] \u003d listdate\n    \n    result \u003d df_pred.to_json(orient\u003d\"index\")\n    parsed \u003d json.loads(result)\n\n\n    return parsed\n",
      "id": "forecast_5",
      "type": "PY_FUNCTION"
    }
  ],
  "publicAccess": true,
  "authRealm": {
    "queryKeys": []
  },
  "tags": [],
  "customFields": {},
  "checklists": {
    "checklists": []
  }
}