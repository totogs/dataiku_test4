{
  "metadata": {
    "kernelspec": {
      "name": "py-dku-containerized-venv-asl_alphabet-gpu_gke_2",
      "display_name": "Python in gpu_gke_2 (env asl_alphabet)",
      "language": "python"
    },
    "associatedRecipe": "compute_predictions",
    "creator": "tony",
    "createdOn": 1629799405892,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {},
    "modifiedBy": "tony"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# -*- coding: utf-8 -*-\nimport dataiku\nimport pandas as pd, numpy as np\nfrom dataiku import pandasutils as pdu\n\n# Read recipe inputs\njson_prepared \u003d dataiku.Dataset(\"Json_prepared\")\ndf \u003d json_prepared.get_dataframe()\n\n\n"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import math\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import Sequence\nfrom datetime import timedelta\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n\nimport numpy as np\nimport pandas as pd\nimport time\n\nimport os"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# Split into training, validation and test datasets.\n# Since it\u0027s timeseries we should do it by date.\ntest_cutoff_date \u003d df[\u0027date_time\u0027].max() - timedelta(days\u003d7)\nval_cutoff_date \u003d test_cutoff_date - timedelta(days\u003d14)\n\ndf_test \u003d df[df[\u0027date_time\u0027] \u003e test_cutoff_date]\ndf_val \u003d df[(df[\u0027date_time\u0027] \u003e val_cutoff_date) \u0026 (df[\u0027date_time\u0027] \u003c\u003d test_cutoff_date)]\ndf_train \u003d df[df[\u0027date_time\u0027] \u003c\u003d val_cutoff_date]\n\n#check out the datasets\nprint(\u0027Test dates: {} to {}\u0027.format(df_test[\u0027date_time\u0027].min(), df_test[\u0027date_time\u0027].max()))\nprint(\u0027Validation dates: {} to {}\u0027.format(df_val[\u0027date_time\u0027].min(), df_val[\u0027date_time\u0027].max()))\nprint(\u0027Train dates: {} to {}\u0027.format(df_train[\u0027date_time\u0027].min(), df_train[\u0027date_time\u0027].max()))"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n# Goal of the model:\n#  Predict Global_active_power at a specified time in the future.\n#   Eg. We want to predict how much Global_active_power will be ten minutes from now.\n#       We can use all the values from t-1, t-2, t-3, .... t-history_length to predict t+10\n\n\ndef create_ts_files(dataset, \n                    start_index, \n                    end_index, \n                    history_length, \n                    step_size, \n                    target_step, \n                    num_rows_per_file, \n                    data_folder):\n    assert step_size \u003e 0\n    assert start_index \u003e\u003d 0\n    \n    if not os.path.exists(data_folder):\n        os.makedirs(data_folder)\n    \n    time_lags \u003d sorted(range(target_step+1, target_step+history_length+1, step_size), reverse\u003dTrue)\n    col_names \u003d [f\u0027x_lag{i}\u0027 for i in time_lags] + [\u0027y\u0027]\n    start_index \u003d start_index + history_length\n    if end_index is None:\n        end_index \u003d len(dataset) - target_step\n    \n    rng \u003d range(start_index, end_index)\n    num_rows \u003d len(rng)\n    num_files \u003d math.ceil(num_rows/num_rows_per_file)\n    \n    # for each file.\n    print(f\u0027Creating {num_files} files.\u0027)\n    for i in range(num_files):\n        filename \u003d f\u0027{data_folder}/ts_file{i}.pkl\u0027\n        \n        if i % 10 \u003d\u003d 0:\n            print(f\u0027{filename}\u0027)\n            \n        # get the start and end indices.\n        ind0 \u003d i*num_rows_per_file\n        ind1 \u003d min(ind0 + num_rows_per_file, end_index)\n        data_list \u003d []\n        \n        # j in the current timestep. Will need j-n to j-1 for the history. And j + target_step for the target.\n        for j in range(ind0, ind1):\n            indices \u003d range(j-1, j-history_length-1, -step_size)\n            data \u003d dataset[sorted(indices) + [j+target_step]]\n            \n            # append data to the list.\n            data_list.append(data)\n\n        df_ts \u003d pd.DataFrame(data\u003ddata_list, columns\u003dcol_names)\n        df_ts.to_pickle(filename)\n            \n    return len(col_names)-1"
      ],
      "outputs": []
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Compute recipe outputs from inputs\n# TODO: Replace this part by your actual code that computes the output, as a Pandas dataframe\n# NB: DSS also supports other kinds of APIs for reading and writing data. Please see doc.\n\npredictions_df \u003d json_prepared_df # For this sample code, simply copy input to output\n\n\n# Write recipe outputs\npredictions \u003d dataiku.Dataset(\"predictions\")\npredictions.write_with_schema(predictions_df)"
      ],
      "outputs": []
    }
  ]
}