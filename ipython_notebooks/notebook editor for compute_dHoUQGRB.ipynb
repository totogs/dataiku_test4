{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python in gpu_gke_2 (env asl_alphabet)",
      "language": "python",
      "name": "py-dku-containerized-venv-asl_alphabet-gpu_gke_2"
    },
    "associatedRecipe": "compute_dHoUQGRB",
    "creator": "khalil",
    "createdOn": 1629822034892,
    "tags": [
      "recipe-editor"
    ],
    "customFields": {}
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import dataiku\n",
        "import pandas as pd, numpy as np\n",
        "from dataiku import pandasutils as pdu\n",
        "\n",
        "# Read recipe inputs\n",
        "json_prepared \u003d dataiku.Dataset(\"Json_prepared\")\n",
        "df \u003d json_prepared.get_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from datetime import timedelta\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "del df[\u0027Timestamp\u0027]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Split into training, validation and test datasets.\n",
        "# Since it\u0027s timeseries we should do it by date.\n",
        "test_cutoff_date \u003d df[\u0027date\u0027].max() - timedelta(days\u003d7)\n",
        "\n",
        "test_df \u003d df[df[\u0027date\u0027] \u003e test_cutoff_date]\n",
        "train_df \u003d df[df[\u0027date\u0027] \u003c\u003d test_cutoff_date]\n",
        "\n",
        "#check out the datasets\n",
        "print(\u0027Test dates: {} to {}\u0027.format(test_df[\u0027date\u0027].min(), test_df[\u0027date\u0027].max()))\n",
        "print(\u0027Train dates: {} to {}\u0027.format(train_df[\u0027date\u0027].min(), train_df[\u0027date\u0027].max()))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "train_df  \u003d train_df.set_index(\u0027date\u0027)\n",
        "test_df \u003d test_df.set_index(\u0027date\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "train \u003d train_df\n",
        "scalers\u003d{}\n",
        "for i in train_df.columns:\n",
        "    scaler \u003d MinMaxScaler(feature_range\u003d(-1,1))\n",
        "    s_s \u003d scaler.fit_transform(train[i].values.reshape(-1,1))\n",
        "    s_s\u003dnp.reshape(s_s,len(s_s))\n",
        "    scalers[\u0027scaler_\u0027+ i] \u003d scaler\n",
        "    train[i]\u003ds_s\n",
        "test \u003d test_df\n",
        "for i in test_df.columns:\n",
        "    scaler \u003d scalers[\u0027scaler_\u0027+i]\n",
        "    s_s \u003d scaler.transform(test[i].values.reshape(-1,1))\n",
        "    s_s\u003dnp.reshape(s_s,len(s_s))\n",
        "    scalers[\u0027scaler_\u0027+i] \u003d scaler\n",
        "    test[i]\u003ds_s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "def split_series(series, n_past, n_future):\n",
        "  #\n",
        "  # n_past \u003d\u003d\u003e no of past observations\n",
        "  #\n",
        "  # n_future \u003d\u003d\u003e no of future observations\n",
        "  #\n",
        "  X, y \u003d list(), list()\n",
        "  for window_start in range(len(series)):\n",
        "    past_end \u003d window_start + n_past\n",
        "    future_end \u003d past_end + n_future\n",
        "    if future_end \u003e len(series):\n",
        "      break\n",
        "    # slicing the past and future parts of the window\n",
        "    past, future \u003d series[window_start:past_end, :], series[past_end:future_end, :]\n",
        "    X.append(past)\n",
        "    y.append(future)\n",
        "  return np.array(X), np.array(y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "n_past \u003d 10\n",
        "n_future \u003d 5\n",
        "n_features \u003d 30"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "X_train, y_train \u003d split_series(train.values,n_past, n_future)\n",
        "X_train \u003d X_train.reshape((X_train.shape[0], X_train.shape[1],n_features))\n",
        "y_train \u003d y_train.reshape((y_train.shape[0], y_train.shape[1], n_features))\n",
        "X_test, y_test \u003d split_series(test.values,n_past, n_future)\n",
        "X_test \u003d X_test.reshape((X_test.shape[0], X_test.shape[1],n_features))\n",
        "y_test \u003d y_test.reshape((y_test.shape[0], y_test.shape[1], n_features))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "encoder_inputs \u003d tf.keras.layers.Input(shape\u003d(n_past, n_features))\n",
        "encoder_l1 \u003d tf.keras.layers.LSTM(100, return_state\u003dTrue)\n",
        "encoder_outputs1 \u003d encoder_l1(encoder_inputs)\n",
        "\n",
        "encoder_states1 \u003d encoder_outputs1[1:]\n",
        "\n",
        "#\n",
        "decoder_inputs \u003d tf.keras.layers.RepeatVector(n_future)(encoder_outputs1[0])\n",
        "\n",
        "#\n",
        "decoder_l1 \u003d tf.keras.layers.LSTM(100, return_sequences\u003dTrue)(decoder_inputs,initial_state \u003d encoder_states1)\n",
        "decoder_outputs1 \u003d tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_features))(decoder_l1)\n",
        "\n",
        "#\n",
        "model_e1d1 \u003d tf.keras.models.Model(encoder_inputs,decoder_outputs1)\n",
        "\n",
        "#\n",
        "model_e1d1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "reduce_lr \u003d tf.keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
        "model_e1d1.compile(optimizer\u003dtf.keras.optimizers.Adam(), loss\u003dtf.keras.losses.Huber())\n",
        "history_e1d1\u003dmodel_e1d1.fit(X_train,y_train,epochs\u003d25,validation_data\u003d(X_test,y_test),batch_size\u003d32,verbose\u003d0,callbacks\u003d[reduce_lr])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "pred_e1d1\u003dmodel_e1d1.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "for index,i in enumerate(train_df.columns):\n",
        "    scaler \u003d scalers[\u0027scaler_\u0027+i]\n",
        "    pred_e1d1[:,:,index]\u003dscaler.inverse_transform(pred_e1d1[:,:,index])\n",
        "    y_train[:,:,index]\u003dscaler.inverse_transform(y_train[:,:,index])\n",
        "    y_test[:,:,index]\u003dscaler.inverse_transform(y_test[:,:,index])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "pred_e1d1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "y_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "pred_e1d1.shape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "json_config \u003d model_e1d1.to_json()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "outputs": [],
      "execution_count": 0,
      "source": [
        "# Write recipe outputs\n",
        "model_forecast \u003d dataiku.Folder(\"dHoUQGRB\")\n",
        "model_forecast_info \u003d model_forecast.get_info()\n",
        "\n",
        "model_forecast.write_json(\"model_json\", json_config)"
      ]
    }
  ]
}